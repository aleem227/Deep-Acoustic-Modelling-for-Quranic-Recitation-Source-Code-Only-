{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import librosa\n",
    "from librosa.util import fix_length\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "from IPython.display import Audio\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(253702,)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr = 16000\n",
    "audio_file_path='audio_wav_fold/fold112/112-al-ikhlas_4.wav'\n",
    "librosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path, sr= 16000)\n",
    "\n",
    "librosa_audio_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VOICE ACTIVITY DETECTION USING SILERO-VAD MODEL & EXTRACT START-END POINTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\aleem/.cache\\torch\\hub\\master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 44000, 'start': 1568},\n",
      " {'end': 92128, 'start': 51232},\n",
      " {'end': 132576, 'start': 99360},\n",
      " {'end': 180192, 'start': 137760},\n",
      " {'end': 246240, 'start': 184864}]\n",
      "The values corresponding to Start : [1568, 51232, 99360, 137760, 184864]\n",
      "The values corresponding to End : [44000, 92128, 132576, 180192, 246240]\n"
     ]
    }
   ],
   "source": [
    "SAMPLING_RATE = 16000\n",
    "\n",
    "USE_ONNX = False # change this to True if you want to test onnx model\n",
    "if USE_ONNX:\n",
    "    !pip install -q onnxruntime\n",
    "    \n",
    " \n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=USE_ONNX)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils\n",
    "\n",
    "\n",
    "# get speech timestamps from full audio file\n",
    "speech_timestamps = get_speech_timestamps(librosa_audio_data, model, sampling_rate=SAMPLING_RATE)\n",
    "pprint(speech_timestamps)\n",
    "\n",
    "################################# Seperate START and END points from Speech_timpstamps ##########################################\n",
    "\n",
    "# Using list comprehension\n",
    "# Get values of particular key in list of dictionaries\n",
    "start = [ sub['start'] for sub in speech_timestamps ]\n",
    " \n",
    "# printing result\n",
    "print(\"The values corresponding to Start : \" + str(start))\n",
    "\n",
    "# Using list comprehension\n",
    "# Get values of particular key in list of dictionaries\n",
    "end = [ sub['end'] for sub in speech_timestamps ]\n",
    " \n",
    "# printing result\n",
    "print(\"The values corresponding to End : \" + str(end))\n",
    "\n",
    "start = np.array(start)\n",
    "end = np.array(end)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACTING AMPLITUDES IMPORTANT FOR US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_amplitudes(audio_data,length,start,end):\n",
    "    \n",
    "    extracted_librosa_audio_data = []\n",
    "    \n",
    "    for i in range(length):\n",
    "        extracted_librosa_audio_data.append([audio_data[start[i]:end[i]]])\n",
    "    return np.array(extracted_librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleem\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_librosa_audio_data = extract_amplitudes(librosa_audio_data,len(start),start,end)\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(extracted_librosa_audio_data)\n",
    "# df.fillna(0).values.tolist()\n",
    "extracted_librosa_audio_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE VAD FILES AS A SEPERATE .WAV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file4_1.wav', 'file4_2.wav', 'file4_3.wav', 'file4_4.wav', 'file4_5.wav']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io.wavfile\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "rate = 16000\n",
    "\n",
    "filenames = []\n",
    "\n",
    "\n",
    "n=len(start)\n",
    "\n",
    "for i in range(1,len(start)+1):\n",
    "    filename = \"file4_{}.wav\".format(i)\n",
    "    filenames.append(filename)\n",
    "    \n",
    "print(filenames)\n",
    "\n",
    "\n",
    "for count,i in enumerate(range(len(start)),1):\n",
    "    savefile = extracted_librosa_audio_data[i][0]\n",
    "    filename = filenames[i]\n",
    "    write(filename = 'extracted_audio_wav_fold/fold112/'+ filename, rate = rate, data = savefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
