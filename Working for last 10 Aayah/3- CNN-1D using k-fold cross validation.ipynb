{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from IPython import display\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('mfcc_X_data.npy')\n",
    "Y = np.load('mfcc_Y_data.npy', allow_pickle='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An-Nas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An-Nas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An-Nas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An-Nas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An-Nas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0  An-Nas\n",
       "1  An-Nas\n",
       "2  An-Nas\n",
       "3  An-Nas\n",
       "4  An-Nas"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_df = pd.DataFrame(Y)\n",
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-221.180679</td>\n",
       "      <td>126.025711</td>\n",
       "      <td>-50.995396</td>\n",
       "      <td>47.782433</td>\n",
       "      <td>-17.575918</td>\n",
       "      <td>28.504759</td>\n",
       "      <td>-16.543465</td>\n",
       "      <td>4.723109</td>\n",
       "      <td>-16.000511</td>\n",
       "      <td>-13.612542</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.393832</td>\n",
       "      <td>-3.856801</td>\n",
       "      <td>1.254481</td>\n",
       "      <td>6.601337</td>\n",
       "      <td>8.466876</td>\n",
       "      <td>15.015834</td>\n",
       "      <td>4.340286</td>\n",
       "      <td>12.090161</td>\n",
       "      <td>12.014910</td>\n",
       "      <td>26.821383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-211.255005</td>\n",
       "      <td>119.753944</td>\n",
       "      <td>-52.348122</td>\n",
       "      <td>63.510727</td>\n",
       "      <td>-21.535557</td>\n",
       "      <td>29.757116</td>\n",
       "      <td>-18.222822</td>\n",
       "      <td>7.571888</td>\n",
       "      <td>-12.699204</td>\n",
       "      <td>-14.380175</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.207137</td>\n",
       "      <td>-4.994766</td>\n",
       "      <td>-0.988067</td>\n",
       "      <td>6.583804</td>\n",
       "      <td>10.860059</td>\n",
       "      <td>22.726280</td>\n",
       "      <td>10.932115</td>\n",
       "      <td>10.071551</td>\n",
       "      <td>1.016532</td>\n",
       "      <td>5.640285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-212.992538</td>\n",
       "      <td>111.009041</td>\n",
       "      <td>-51.069992</td>\n",
       "      <td>55.615635</td>\n",
       "      <td>-29.020407</td>\n",
       "      <td>37.870895</td>\n",
       "      <td>-22.575338</td>\n",
       "      <td>4.039803</td>\n",
       "      <td>-10.614483</td>\n",
       "      <td>-17.730742</td>\n",
       "      <td>...</td>\n",
       "      <td>3.250622</td>\n",
       "      <td>3.888988</td>\n",
       "      <td>8.326393</td>\n",
       "      <td>7.663583</td>\n",
       "      <td>4.940736</td>\n",
       "      <td>5.889103</td>\n",
       "      <td>-3.204108</td>\n",
       "      <td>9.284558</td>\n",
       "      <td>7.726558</td>\n",
       "      <td>22.162859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-252.801468</td>\n",
       "      <td>135.314240</td>\n",
       "      <td>-43.591640</td>\n",
       "      <td>50.476078</td>\n",
       "      <td>-1.697368</td>\n",
       "      <td>27.965115</td>\n",
       "      <td>-11.784382</td>\n",
       "      <td>10.756843</td>\n",
       "      <td>-10.194268</td>\n",
       "      <td>-16.072851</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.171225</td>\n",
       "      <td>-7.709264</td>\n",
       "      <td>-2.342036</td>\n",
       "      <td>8.532597</td>\n",
       "      <td>15.277192</td>\n",
       "      <td>21.758413</td>\n",
       "      <td>5.262423</td>\n",
       "      <td>8.937285</td>\n",
       "      <td>7.269296</td>\n",
       "      <td>15.028630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-155.234695</td>\n",
       "      <td>131.976410</td>\n",
       "      <td>-41.808273</td>\n",
       "      <td>38.996029</td>\n",
       "      <td>-39.036037</td>\n",
       "      <td>46.623611</td>\n",
       "      <td>-22.237730</td>\n",
       "      <td>-8.598266</td>\n",
       "      <td>-6.576684</td>\n",
       "      <td>-20.480564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876611</td>\n",
       "      <td>-1.357790</td>\n",
       "      <td>-15.314865</td>\n",
       "      <td>3.491620</td>\n",
       "      <td>-9.598725</td>\n",
       "      <td>-5.994679</td>\n",
       "      <td>0.465712</td>\n",
       "      <td>-9.627331</td>\n",
       "      <td>-9.393802</td>\n",
       "      <td>-8.305319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1          2          3          4          5   \\\n",
       "0 -221.180679  126.025711 -50.995396  47.782433 -17.575918  28.504759   \n",
       "1 -211.255005  119.753944 -52.348122  63.510727 -21.535557  29.757116   \n",
       "2 -212.992538  111.009041 -51.069992  55.615635 -29.020407  37.870895   \n",
       "3 -252.801468  135.314240 -43.591640  50.476078  -1.697368  27.965115   \n",
       "4 -155.234695  131.976410 -41.808273  38.996029 -39.036037  46.623611   \n",
       "\n",
       "          6          7          8          9   ...         30        31  \\\n",
       "0 -16.543465   4.723109 -16.000511 -13.612542  ...  -6.393832 -3.856801   \n",
       "1 -18.222822   7.571888 -12.699204 -14.380175  ...  -6.207137 -4.994766   \n",
       "2 -22.575338   4.039803 -10.614483 -17.730742  ...   3.250622  3.888988   \n",
       "3 -11.784382  10.756843 -10.194268 -16.072851  ... -10.171225 -7.709264   \n",
       "4 -22.237730  -8.598266  -6.576684 -20.480564  ...   0.876611 -1.357790   \n",
       "\n",
       "          32        33         34         35         36         37         38  \\\n",
       "0   1.254481  6.601337   8.466876  15.015834   4.340286  12.090161  12.014910   \n",
       "1  -0.988067  6.583804  10.860059  22.726280  10.932115  10.071551   1.016532   \n",
       "2   8.326393  7.663583   4.940736   5.889103  -3.204108   9.284558   7.726558   \n",
       "3  -2.342036  8.532597  15.277192  21.758413   5.262423   8.937285   7.269296   \n",
       "4 -15.314865  3.491620  -9.598725  -5.994679   0.465712  -9.627331  -9.393802   \n",
       "\n",
       "          39  \n",
       "0  26.821383  \n",
       "1   5.640285  \n",
       "2  22.162859  \n",
       "3  15.028630  \n",
       "4  -8.305319  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = pd.DataFrame(X)\n",
    "X_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unison Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "\n",
    "\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = numpy.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = numpy.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = numpy.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre, y_pre = shuffle_in_unison(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1210, 40)\n",
      "(1210,)\n"
     ]
    }
   ],
   "source": [
    "print(X_pre.shape)\n",
    "print(y_pre.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding for 10 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['An-Nas', 'Al-Falaq','Al-Fil','Quraish','Al-Maun','Al-Kauthar','Al-Kafirun','An-Nasr','Al-Masad','Al-Ikhlas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_enc = np.array([labels.index(l) for l in y_pre])\n",
    "y_enc = np.array(tf.one_hot(y_enc, len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1210, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_enc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_split(X, y, test_size):\n",
    "#     n_test = int(X.shape[0] * test_size)\n",
    "#     X_test = X[:n_test]\n",
    "#     y_test = y[:n_test]\n",
    "#     X_train = X[n_test:]\n",
    "#     y_train = y[n_test:]\n",
    "#     return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_pre, y_enc, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = shuffle_in_unison (X_train, y_train)\n",
    "# X_test, y_test = shuffle_in_unison (X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand Dimension for CNN-1D, CNN-2D and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.expand_dims(X_train, axis=2)\n",
    "# X_test = np.expand_dims(X_test, axis = 2)\n",
    "\n",
    "# print(\"Shape of X Train\", X_train.shape)\n",
    "# print(\"Shape of X Test\", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape for Only Transformer Models and CNN-2D Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_reshaped = X_train.reshape(X_train.shape[0], 10, 4, 1)\n",
    "# X_test_reshaped = X_test.reshape(X_test.shape[0], 10, 4, 1)\n",
    "\n",
    "# print(X_train_reshaped.shape, y_train.shape)\n",
    "# print(X_test_reshaped.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  CNN 1D Model For 10 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import os, time, warnings\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    ")\n",
    "\n",
    "from keras import regularizers\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 40, 256)           1536      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 40, 256)          1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 20, 256)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 20, 256)           327936    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 256)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 10, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 10, 128)           163968    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 5, 128)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 5, 64)             41024     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 3, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 192)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              197632    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 743,370\n",
      "Trainable params: 742,858\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN1D_Model = Sequential()\n",
    "CNN1D_Model.add(\n",
    "    Conv1D(\n",
    "        256,\n",
    "        5,\n",
    "        strides=1,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=(X_pre.shape[1], 1),\n",
    "    )\n",
    ")\n",
    "CNN1D_Model.add(BatchNormalization())\n",
    "CNN1D_Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "CNN1D_Model.add(Conv1D(256, 5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "CNN1D_Model.add(Dropout(0.3))\n",
    "CNN1D_Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "CNN1D_Model.add(Conv1D(128, 5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "CNN1D_Model.add(Dropout(0.3))\n",
    "CNN1D_Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "CNN1D_Model.add(Conv1D(64, 5, strides=1, padding=\"same\", activation=\"relu\"))\n",
    "CNN1D_Model.add(Dropout(0.3))\n",
    "CNN1D_Model.add(MaxPooling1D(3, strides=2, padding=\"same\"))\n",
    "CNN1D_Model.add(Flatten())\n",
    "CNN1D_Model.add(Dense(units=1024, activation=\"relu\"))\n",
    "CNN1D_Model.add(Dropout(0.3))\n",
    "CNN1D_Model.add(Dense(units=10, activation=\"softmax\"))\n",
    "CNN1D_Model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN1D_Model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/80\n",
      "26/26 [==============================] - 4s 77ms/step - loss: 2.4828 - accuracy: 0.1365 - val_loss: 2.3173 - val_accuracy: 0.1015\n",
      "Epoch 2/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 2.2766 - accuracy: 0.1526 - val_loss: 2.2916 - val_accuracy: 0.1089\n",
      "Epoch 3/80\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 2.1980 - accuracy: 0.1886 - val_loss: 2.2152 - val_accuracy: 0.2104\n",
      "Epoch 4/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 2.1401 - accuracy: 0.2283 - val_loss: 2.1858 - val_accuracy: 0.1708\n",
      "Epoch 5/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 2.0737 - accuracy: 0.2035 - val_loss: 2.1259 - val_accuracy: 0.2401\n",
      "Epoch 6/80\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 2.0543 - accuracy: 0.2333 - val_loss: 1.9920 - val_accuracy: 0.2748\n",
      "Epoch 7/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 2.0016 - accuracy: 0.2643 - val_loss: 1.9782 - val_accuracy: 0.3292\n",
      "Epoch 8/80\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 1.8769 - accuracy: 0.3201 - val_loss: 1.7504 - val_accuracy: 0.3614\n",
      "Epoch 9/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 1.8380 - accuracy: 0.3176 - val_loss: 1.8042 - val_accuracy: 0.3936\n",
      "Epoch 10/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 1.7949 - accuracy: 0.3424 - val_loss: 1.7464 - val_accuracy: 0.3490\n",
      "Epoch 11/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 1.7058 - accuracy: 0.3623 - val_loss: 1.7077 - val_accuracy: 0.3564\n",
      "Epoch 12/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 1.5964 - accuracy: 0.4082 - val_loss: 1.5910 - val_accuracy: 0.3985\n",
      "Epoch 13/80\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 1.6392 - accuracy: 0.3908 - val_loss: 1.7482 - val_accuracy: 0.3886\n",
      "Epoch 14/80\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 1.5815 - accuracy: 0.4218 - val_loss: 1.4965 - val_accuracy: 0.4678\n",
      "Epoch 15/80\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 1.4220 - accuracy: 0.4864 - val_loss: 1.4055 - val_accuracy: 0.5297\n",
      "Epoch 16/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 1.3561 - accuracy: 0.5062 - val_loss: 1.3378 - val_accuracy: 0.5594\n",
      "Epoch 17/80\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 1.3062 - accuracy: 0.5223 - val_loss: 1.3909 - val_accuracy: 0.5124\n",
      "Epoch 18/80\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 1.3176 - accuracy: 0.5050 - val_loss: 1.3140 - val_accuracy: 0.5718\n",
      "Epoch 19/80\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 1.2764 - accuracy: 0.5273 - val_loss: 1.2708 - val_accuracy: 0.6163\n",
      "Epoch 20/80\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 1.1706 - accuracy: 0.5583 - val_loss: 1.1447 - val_accuracy: 0.6114\n",
      "Epoch 21/80\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 1.0063 - accuracy: 0.6315 - val_loss: 1.1397 - val_accuracy: 0.6436\n",
      "Epoch 22/80\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 1.1142 - accuracy: 0.6154 - val_loss: 1.1198 - val_accuracy: 0.6460\n",
      "Epoch 23/80\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 1.0651 - accuracy: 0.6266 - val_loss: 1.0023 - val_accuracy: 0.6609\n",
      "Epoch 24/80\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 0.9638 - accuracy: 0.6452 - val_loss: 1.0425 - val_accuracy: 0.6535\n",
      "Epoch 25/80\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 0.9469 - accuracy: 0.6712 - val_loss: 1.0231 - val_accuracy: 0.6634\n",
      "Epoch 26/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.8689 - accuracy: 0.6861 - val_loss: 0.9074 - val_accuracy: 0.7228\n",
      "Epoch 27/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.7845 - accuracy: 0.7221 - val_loss: 0.8920 - val_accuracy: 0.7079\n",
      "Epoch 28/80\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 0.8282 - accuracy: 0.6886 - val_loss: 0.9833 - val_accuracy: 0.6906\n",
      "Epoch 29/80\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 0.7200 - accuracy: 0.7345 - val_loss: 0.8575 - val_accuracy: 0.7178\n",
      "Epoch 30/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.7359 - accuracy: 0.7395 - val_loss: 0.7908 - val_accuracy: 0.7302\n",
      "Epoch 31/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.6230 - accuracy: 0.7717 - val_loss: 0.7680 - val_accuracy: 0.7649\n",
      "Epoch 32/80\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 0.7446 - accuracy: 0.7308 - val_loss: 0.8212 - val_accuracy: 0.7228\n",
      "Epoch 33/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.7472 - accuracy: 0.7382 - val_loss: 0.7893 - val_accuracy: 0.7525\n",
      "Epoch 34/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.6648 - accuracy: 0.7605 - val_loss: 0.7432 - val_accuracy: 0.7500\n",
      "Epoch 35/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.6510 - accuracy: 0.7730 - val_loss: 0.7110 - val_accuracy: 0.7946\n",
      "Epoch 36/80\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 0.5730 - accuracy: 0.8077 - val_loss: 0.7319 - val_accuracy: 0.7673\n",
      "Epoch 37/80\n",
      "26/26 [==============================] - 2s 72ms/step - loss: 0.5346 - accuracy: 0.8176 - val_loss: 0.9002 - val_accuracy: 0.7203\n",
      "Epoch 38/80\n",
      "26/26 [==============================] - 2s 71ms/step - loss: 0.6090 - accuracy: 0.7990 - val_loss: 0.7173 - val_accuracy: 0.7772\n",
      "Epoch 39/80\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 0.4594 - accuracy: 0.8263 - val_loss: 0.6870 - val_accuracy: 0.7946\n",
      "Epoch 40/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.5280 - accuracy: 0.8313 - val_loss: 0.6316 - val_accuracy: 0.8119\n",
      "Epoch 41/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.4869 - accuracy: 0.8337 - val_loss: 0.6653 - val_accuracy: 0.7871\n",
      "Epoch 42/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.4058 - accuracy: 0.8610 - val_loss: 0.6718 - val_accuracy: 0.7921\n",
      "Epoch 43/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.5522 - accuracy: 0.8300 - val_loss: 0.6849 - val_accuracy: 0.7995\n",
      "Epoch 44/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.4908 - accuracy: 0.8325 - val_loss: 0.6697 - val_accuracy: 0.7921\n",
      "Epoch 45/80\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 0.4507 - accuracy: 0.8362 - val_loss: 0.5768 - val_accuracy: 0.8292\n",
      "Epoch 46/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.4337 - accuracy: 0.8437 - val_loss: 0.6139 - val_accuracy: 0.8366\n",
      "Epoch 47/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.3474 - accuracy: 0.8747 - val_loss: 0.5645 - val_accuracy: 0.8416\n",
      "Epoch 48/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.3506 - accuracy: 0.8660 - val_loss: 0.5461 - val_accuracy: 0.8441\n",
      "Epoch 49/80\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 0.3162 - accuracy: 0.8896 - val_loss: 0.5610 - val_accuracy: 0.8342\n",
      "Epoch 50/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.3138 - accuracy: 0.8896 - val_loss: 0.5008 - val_accuracy: 0.8391\n",
      "Epoch 51/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.3683 - accuracy: 0.8821 - val_loss: 0.4874 - val_accuracy: 0.8465\n",
      "Epoch 52/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.3771 - accuracy: 0.8772 - val_loss: 0.5887 - val_accuracy: 0.8243\n",
      "Epoch 53/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.3846 - accuracy: 0.8834 - val_loss: 0.5916 - val_accuracy: 0.7946\n",
      "Epoch 54/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.4566 - accuracy: 0.8561 - val_loss: 0.5769 - val_accuracy: 0.8168\n",
      "Epoch 55/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.2714 - accuracy: 0.9094 - val_loss: 0.5395 - val_accuracy: 0.8292\n",
      "Epoch 56/80\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 0.2672 - accuracy: 0.9082 - val_loss: 0.5223 - val_accuracy: 0.8564\n",
      "Epoch 57/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.2639 - accuracy: 0.9057 - val_loss: 0.5491 - val_accuracy: 0.8614\n",
      "Epoch 58/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.3428 - accuracy: 0.8933 - val_loss: 0.5303 - val_accuracy: 0.8589\n",
      "Epoch 59/80\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 0.2466 - accuracy: 0.9181 - val_loss: 0.5173 - val_accuracy: 0.8614\n",
      "Epoch 60/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.2621 - accuracy: 0.9107 - val_loss: 0.5424 - val_accuracy: 0.8317\n",
      "Epoch 61/80\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 0.3148 - accuracy: 0.9045 - val_loss: 0.5089 - val_accuracy: 0.8465\n",
      "Epoch 62/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.2279 - accuracy: 0.9293 - val_loss: 0.5233 - val_accuracy: 0.8490\n",
      "Epoch 63/80\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 0.2271 - accuracy: 0.9243 - val_loss: 0.4866 - val_accuracy: 0.8515\n",
      "Epoch 64/80\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 0.1859 - accuracy: 0.9454 - val_loss: 0.4303 - val_accuracy: 0.8688\n",
      "Epoch 65/80\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 0.2527 - accuracy: 0.9144 - val_loss: 0.4900 - val_accuracy: 0.8589\n",
      "Epoch 66/80\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 0.2576 - accuracy: 0.9057 - val_loss: 0.4832 - val_accuracy: 0.8564\n",
      "Epoch 67/80\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 0.2200 - accuracy: 0.9318 - val_loss: 0.4489 - val_accuracy: 0.8762\n",
      "Epoch 68/80\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 0.2654 - accuracy: 0.9156 - val_loss: 0.4454 - val_accuracy: 0.8688\n",
      "Epoch 69/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.2497 - accuracy: 0.9206 - val_loss: 0.5748 - val_accuracy: 0.8441\n",
      "Epoch 70/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.3134 - accuracy: 0.9045 - val_loss: 0.4912 - val_accuracy: 0.8663\n",
      "Epoch 71/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.2535 - accuracy: 0.9231 - val_loss: 0.4682 - val_accuracy: 0.8441\n",
      "Epoch 72/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.2791 - accuracy: 0.9094 - val_loss: 0.4126 - val_accuracy: 0.8738\n",
      "Epoch 73/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.2279 - accuracy: 0.9256 - val_loss: 0.4594 - val_accuracy: 0.8515\n",
      "Epoch 74/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.1794 - accuracy: 0.9318 - val_loss: 0.4190 - val_accuracy: 0.8663\n",
      "Epoch 75/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1767 - accuracy: 0.9330 - val_loss: 0.4433 - val_accuracy: 0.8738\n",
      "Epoch 76/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.2154 - accuracy: 0.9392 - val_loss: 0.4365 - val_accuracy: 0.8738\n",
      "Epoch 77/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.2202 - accuracy: 0.9218 - val_loss: 0.4685 - val_accuracy: 0.8812\n",
      "Epoch 78/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.3894 - accuracy: 0.8958 - val_loss: 0.4803 - val_accuracy: 0.8589\n",
      "Epoch 79/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.2489 - accuracy: 0.9181 - val_loss: 0.4856 - val_accuracy: 0.8465\n",
      "Epoch 80/80\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 0.2313 - accuracy: 0.9156 - val_loss: 0.5354 - val_accuracy: 0.8441\n",
      "Fold 2\n",
      "Epoch 1/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.4899 - accuracy: 0.8538 - val_loss: 0.1744 - val_accuracy: 0.9529\n",
      "Epoch 2/80\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 0.5562 - accuracy: 0.8340 - val_loss: 0.2444 - val_accuracy: 0.9479\n",
      "Epoch 3/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.4903 - accuracy: 0.8501 - val_loss: 0.2478 - val_accuracy: 0.9578\n",
      "Epoch 4/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.3874 - accuracy: 0.8724 - val_loss: 0.1893 - val_accuracy: 0.9603\n",
      "Epoch 5/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.3563 - accuracy: 0.8823 - val_loss: 0.1680 - val_accuracy: 0.9677\n",
      "Epoch 6/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.2613 - accuracy: 0.9108 - val_loss: 0.1986 - val_accuracy: 0.9504\n",
      "Epoch 7/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.3158 - accuracy: 0.8947 - val_loss: 0.1884 - val_accuracy: 0.9578\n",
      "Epoch 8/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.1934 - accuracy: 0.9318 - val_loss: 0.1578 - val_accuracy: 0.9628\n",
      "Epoch 9/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.2837 - accuracy: 0.9095 - val_loss: 0.1567 - val_accuracy: 0.9677\n",
      "Epoch 10/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.2539 - accuracy: 0.9195 - val_loss: 0.1429 - val_accuracy: 0.9653\n",
      "Epoch 11/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.2166 - accuracy: 0.9257 - val_loss: 0.1662 - val_accuracy: 0.9479\n",
      "Epoch 12/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.2998 - accuracy: 0.9046 - val_loss: 0.1698 - val_accuracy: 0.9479\n",
      "Epoch 13/80\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 0.3168 - accuracy: 0.8959 - val_loss: 0.2012 - val_accuracy: 0.9454\n",
      "Epoch 14/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.3654 - accuracy: 0.8885 - val_loss: 0.2154 - val_accuracy: 0.9578\n",
      "Epoch 15/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.3095 - accuracy: 0.9021 - val_loss: 0.1907 - val_accuracy: 0.9529\n",
      "Epoch 16/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.2642 - accuracy: 0.9083 - val_loss: 0.2065 - val_accuracy: 0.9628\n",
      "Epoch 17/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.1998 - accuracy: 0.9356 - val_loss: 0.1645 - val_accuracy: 0.9578\n",
      "Epoch 18/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.1564 - accuracy: 0.9393 - val_loss: 0.1483 - val_accuracy: 0.9727\n",
      "Epoch 19/80\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 0.1517 - accuracy: 0.9492 - val_loss: 0.1340 - val_accuracy: 0.9603\n",
      "Epoch 20/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.1732 - accuracy: 0.9554 - val_loss: 0.1923 - val_accuracy: 0.9529\n",
      "Epoch 21/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1627 - accuracy: 0.9517 - val_loss: 0.1681 - val_accuracy: 0.9553\n",
      "Epoch 22/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.1506 - accuracy: 0.9504 - val_loss: 0.1529 - val_accuracy: 0.9578\n",
      "Epoch 23/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1308 - accuracy: 0.9591 - val_loss: 0.1566 - val_accuracy: 0.9529\n",
      "Epoch 24/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1336 - accuracy: 0.9554 - val_loss: 0.1801 - val_accuracy: 0.9529\n",
      "Epoch 25/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1343 - accuracy: 0.9566 - val_loss: 0.1698 - val_accuracy: 0.9479\n",
      "Epoch 26/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.1512 - accuracy: 0.9517 - val_loss: 0.1737 - val_accuracy: 0.9504\n",
      "Epoch 27/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.1400 - accuracy: 0.9467 - val_loss: 0.1493 - val_accuracy: 0.9578\n",
      "Epoch 28/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.2361 - accuracy: 0.9244 - val_loss: 0.2128 - val_accuracy: 0.9429\n",
      "Epoch 29/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.2282 - accuracy: 0.9318 - val_loss: 0.2321 - val_accuracy: 0.9231\n",
      "Epoch 30/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.2455 - accuracy: 0.9257 - val_loss: 0.2207 - val_accuracy: 0.9305\n",
      "Epoch 31/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.1741 - accuracy: 0.9331 - val_loss: 0.2424 - val_accuracy: 0.9380\n",
      "Epoch 32/80\n",
      "26/26 [==============================] - 2s 71ms/step - loss: 0.2637 - accuracy: 0.9157 - val_loss: 0.2115 - val_accuracy: 0.9504\n",
      "Epoch 33/80\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.2885 - accuracy: 0.9120 - val_loss: 0.2183 - val_accuracy: 0.9479\n",
      "Epoch 34/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.2012 - accuracy: 0.9343 - val_loss: 0.1845 - val_accuracy: 0.9628\n",
      "Epoch 35/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.1820 - accuracy: 0.9455 - val_loss: 0.2557 - val_accuracy: 0.9330\n",
      "Epoch 36/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.1913 - accuracy: 0.9455 - val_loss: 0.2079 - val_accuracy: 0.9429\n",
      "Epoch 37/80\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 0.1533 - accuracy: 0.9418 - val_loss: 0.2285 - val_accuracy: 0.9355\n",
      "Epoch 38/80\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 0.1414 - accuracy: 0.9542 - val_loss: 0.1787 - val_accuracy: 0.9454\n",
      "Epoch 39/80\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 0.1449 - accuracy: 0.9566 - val_loss: 0.1715 - val_accuracy: 0.9504\n",
      "Epoch 40/80\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 0.1546 - accuracy: 0.9616 - val_loss: 0.2393 - val_accuracy: 0.9305\n",
      "Epoch 41/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1654 - accuracy: 0.9566 - val_loss: 0.2172 - val_accuracy: 0.9479\n",
      "Epoch 42/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.1743 - accuracy: 0.9442 - val_loss: 0.2791 - val_accuracy: 0.9330\n",
      "Epoch 43/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1405 - accuracy: 0.9529 - val_loss: 0.1885 - val_accuracy: 0.9479\n",
      "Epoch 44/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1156 - accuracy: 0.9616 - val_loss: 0.2238 - val_accuracy: 0.9529\n",
      "Epoch 45/80\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 0.1428 - accuracy: 0.9554 - val_loss: 0.1848 - val_accuracy: 0.9553\n",
      "Epoch 46/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.1293 - accuracy: 0.9542 - val_loss: 0.2638 - val_accuracy: 0.9355\n",
      "Epoch 47/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.1380 - accuracy: 0.9542 - val_loss: 0.2323 - val_accuracy: 0.9355\n",
      "Epoch 48/80\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 0.1123 - accuracy: 0.9653 - val_loss: 0.3040 - val_accuracy: 0.9132\n",
      "Epoch 49/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1261 - accuracy: 0.9653 - val_loss: 0.2581 - val_accuracy: 0.9231\n",
      "Epoch 50/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.1043 - accuracy: 0.9703 - val_loss: 0.1933 - val_accuracy: 0.9355\n",
      "Epoch 51/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.0740 - accuracy: 0.9715 - val_loss: 0.2242 - val_accuracy: 0.9404\n",
      "Epoch 52/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.0918 - accuracy: 0.9690 - val_loss: 0.2162 - val_accuracy: 0.9479\n",
      "Epoch 53/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1054 - accuracy: 0.9653 - val_loss: 0.2120 - val_accuracy: 0.9429\n",
      "Epoch 54/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1264 - accuracy: 0.9628 - val_loss: 0.2079 - val_accuracy: 0.9479\n",
      "Epoch 55/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.1579 - accuracy: 0.9542 - val_loss: 0.2188 - val_accuracy: 0.9404\n",
      "Epoch 56/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1216 - accuracy: 0.9579 - val_loss: 0.2538 - val_accuracy: 0.9206\n",
      "Epoch 57/80\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 0.0781 - accuracy: 0.9727 - val_loss: 0.1908 - val_accuracy: 0.9404\n",
      "Epoch 58/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1062 - accuracy: 0.9665 - val_loss: 0.2132 - val_accuracy: 0.9479\n",
      "Epoch 59/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1362 - accuracy: 0.9641 - val_loss: 0.2466 - val_accuracy: 0.9280\n",
      "Epoch 60/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.1191 - accuracy: 0.9579 - val_loss: 0.2579 - val_accuracy: 0.9280\n",
      "Epoch 61/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.1388 - accuracy: 0.9653 - val_loss: 0.2690 - val_accuracy: 0.9206\n",
      "Epoch 62/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1391 - accuracy: 0.9542 - val_loss: 0.3066 - val_accuracy: 0.9280\n",
      "Epoch 63/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.0983 - accuracy: 0.9715 - val_loss: 0.2792 - val_accuracy: 0.9404\n",
      "Epoch 64/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1448 - accuracy: 0.9542 - val_loss: 0.2746 - val_accuracy: 0.9305\n",
      "Epoch 65/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.0996 - accuracy: 0.9703 - val_loss: 0.2456 - val_accuracy: 0.9355\n",
      "Epoch 66/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1100 - accuracy: 0.9579 - val_loss: 0.2258 - val_accuracy: 0.9355\n",
      "Epoch 67/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1226 - accuracy: 0.9641 - val_loss: 0.2364 - val_accuracy: 0.9330\n",
      "Epoch 68/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.0917 - accuracy: 0.9703 - val_loss: 0.3093 - val_accuracy: 0.9305\n",
      "Epoch 69/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1068 - accuracy: 0.9603 - val_loss: 0.3190 - val_accuracy: 0.9082\n",
      "Epoch 70/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1951 - accuracy: 0.9554 - val_loss: 0.2771 - val_accuracy: 0.9256\n",
      "Epoch 71/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1007 - accuracy: 0.9703 - val_loss: 0.2756 - val_accuracy: 0.9231\n",
      "Epoch 72/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.0787 - accuracy: 0.9740 - val_loss: 0.2870 - val_accuracy: 0.9280\n",
      "Epoch 73/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.1020 - accuracy: 0.9653 - val_loss: 0.2694 - val_accuracy: 0.9256\n",
      "Epoch 74/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1269 - accuracy: 0.9591 - val_loss: 0.2736 - val_accuracy: 0.9256\n",
      "Epoch 75/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1480 - accuracy: 0.9542 - val_loss: 0.3229 - val_accuracy: 0.9181\n",
      "Epoch 76/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.0795 - accuracy: 0.9715 - val_loss: 0.2562 - val_accuracy: 0.9355\n",
      "Epoch 77/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.1481 - accuracy: 0.9542 - val_loss: 0.2847 - val_accuracy: 0.9280\n",
      "Epoch 78/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.2859 - accuracy: 0.9318 - val_loss: 0.4299 - val_accuracy: 0.9007\n",
      "Epoch 79/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.2646 - accuracy: 0.9269 - val_loss: 0.3408 - val_accuracy: 0.9107\n",
      "Epoch 80/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.2074 - accuracy: 0.9430 - val_loss: 0.2732 - val_accuracy: 0.9231\n",
      "Fold 3\n",
      "Epoch 1/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.3750 - accuracy: 0.9058 - val_loss: 0.0367 - val_accuracy: 0.9926\n",
      "Epoch 2/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.2925 - accuracy: 0.9244 - val_loss: 0.0562 - val_accuracy: 0.9926\n",
      "Epoch 3/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1998 - accuracy: 0.9343 - val_loss: 0.0476 - val_accuracy: 0.9851\n",
      "Epoch 4/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1487 - accuracy: 0.9492 - val_loss: 0.0502 - val_accuracy: 0.9901\n",
      "Epoch 5/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1908 - accuracy: 0.9418 - val_loss: 0.0453 - val_accuracy: 0.9926\n",
      "Epoch 6/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.2242 - accuracy: 0.9306 - val_loss: 0.0867 - val_accuracy: 0.9752\n",
      "Epoch 7/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1758 - accuracy: 0.9418 - val_loss: 0.0525 - val_accuracy: 0.9851\n",
      "Epoch 8/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1834 - accuracy: 0.9467 - val_loss: 0.0453 - val_accuracy: 0.9950\n",
      "Epoch 9/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1975 - accuracy: 0.9405 - val_loss: 0.1012 - val_accuracy: 0.9727\n",
      "Epoch 10/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.2298 - accuracy: 0.9269 - val_loss: 0.0587 - val_accuracy: 0.9926\n",
      "Epoch 11/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1368 - accuracy: 0.9591 - val_loss: 0.0455 - val_accuracy: 0.9950\n",
      "Epoch 12/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1408 - accuracy: 0.9616 - val_loss: 0.0577 - val_accuracy: 0.9851\n",
      "Epoch 13/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.2890 - accuracy: 0.9182 - val_loss: 0.0978 - val_accuracy: 0.9801\n",
      "Epoch 14/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.3007 - accuracy: 0.9257 - val_loss: 0.0809 - val_accuracy: 0.9826\n",
      "Epoch 15/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1637 - accuracy: 0.9393 - val_loss: 0.0746 - val_accuracy: 0.9801\n",
      "Epoch 16/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1570 - accuracy: 0.9467 - val_loss: 0.0561 - val_accuracy: 0.9901\n",
      "Epoch 17/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1409 - accuracy: 0.9579 - val_loss: 0.0666 - val_accuracy: 0.9876\n",
      "Epoch 18/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1721 - accuracy: 0.9542 - val_loss: 0.0608 - val_accuracy: 0.9901\n",
      "Epoch 19/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1193 - accuracy: 0.9566 - val_loss: 0.0749 - val_accuracy: 0.9777\n",
      "Epoch 20/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1053 - accuracy: 0.9641 - val_loss: 0.0847 - val_accuracy: 0.9752\n",
      "Epoch 21/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1277 - accuracy: 0.9579 - val_loss: 0.0926 - val_accuracy: 0.9702\n",
      "Epoch 22/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1763 - accuracy: 0.9492 - val_loss: 0.0790 - val_accuracy: 0.9727\n",
      "Epoch 23/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1663 - accuracy: 0.9591 - val_loss: 0.1198 - val_accuracy: 0.9529\n",
      "Epoch 24/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.1483 - accuracy: 0.9566 - val_loss: 0.1129 - val_accuracy: 0.9653\n",
      "Epoch 25/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1707 - accuracy: 0.9504 - val_loss: 0.0668 - val_accuracy: 0.9901\n",
      "Epoch 26/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.0901 - accuracy: 0.9678 - val_loss: 0.0638 - val_accuracy: 0.9926\n",
      "Epoch 27/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1306 - accuracy: 0.9653 - val_loss: 0.0687 - val_accuracy: 0.9851\n",
      "Epoch 28/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1112 - accuracy: 0.9727 - val_loss: 0.0768 - val_accuracy: 0.9901\n",
      "Epoch 29/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1497 - accuracy: 0.9529 - val_loss: 0.0717 - val_accuracy: 0.9901\n",
      "Epoch 30/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1033 - accuracy: 0.9653 - val_loss: 0.0661 - val_accuracy: 0.9876\n",
      "Epoch 31/80\n",
      "26/26 [==============================] - 2s 71ms/step - loss: 0.0776 - accuracy: 0.9690 - val_loss: 0.0558 - val_accuracy: 0.9926\n",
      "Epoch 32/80\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 0.0719 - accuracy: 0.9752 - val_loss: 0.0741 - val_accuracy: 0.9727\n",
      "Epoch 33/80\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.0798 - accuracy: 0.9740 - val_loss: 0.0609 - val_accuracy: 0.9876\n",
      "Epoch 34/80\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 0.0964 - accuracy: 0.9715 - val_loss: 0.0615 - val_accuracy: 0.9826\n",
      "Epoch 35/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1474 - accuracy: 0.9492 - val_loss: 0.1501 - val_accuracy: 0.9380\n",
      "Epoch 36/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1761 - accuracy: 0.9480 - val_loss: 0.0598 - val_accuracy: 0.9777\n",
      "Epoch 37/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1374 - accuracy: 0.9591 - val_loss: 0.0629 - val_accuracy: 0.9851\n",
      "Epoch 38/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1941 - accuracy: 0.9393 - val_loss: 0.0869 - val_accuracy: 0.9777\n",
      "Epoch 39/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1288 - accuracy: 0.9591 - val_loss: 0.0911 - val_accuracy: 0.9727\n",
      "Epoch 40/80\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 0.1687 - accuracy: 0.9529 - val_loss: 0.1078 - val_accuracy: 0.9752\n",
      "Epoch 41/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.2311 - accuracy: 0.9356 - val_loss: 0.0696 - val_accuracy: 0.9851\n",
      "Epoch 42/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1865 - accuracy: 0.9529 - val_loss: 0.0715 - val_accuracy: 0.9851\n",
      "Epoch 43/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1420 - accuracy: 0.9554 - val_loss: 0.0749 - val_accuracy: 0.9851\n",
      "Epoch 44/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.1773 - accuracy: 0.9418 - val_loss: 0.0993 - val_accuracy: 0.9851\n",
      "Epoch 45/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.2060 - accuracy: 0.9393 - val_loss: 0.0893 - val_accuracy: 0.9901\n",
      "Epoch 46/80\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.1318 - accuracy: 0.9492 - val_loss: 0.0999 - val_accuracy: 0.9653\n",
      "Epoch 47/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1336 - accuracy: 0.9628 - val_loss: 0.0796 - val_accuracy: 0.9801\n",
      "Epoch 48/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.0755 - accuracy: 0.9727 - val_loss: 0.0728 - val_accuracy: 0.9826\n",
      "Epoch 49/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.0655 - accuracy: 0.9777 - val_loss: 0.0840 - val_accuracy: 0.9801\n",
      "Epoch 50/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.0605 - accuracy: 0.9777 - val_loss: 0.0954 - val_accuracy: 0.9752\n",
      "Epoch 51/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.0943 - accuracy: 0.9740 - val_loss: 0.0919 - val_accuracy: 0.9752\n",
      "Epoch 52/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1291 - accuracy: 0.9703 - val_loss: 0.0760 - val_accuracy: 0.9876\n",
      "Epoch 53/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.0830 - accuracy: 0.9740 - val_loss: 0.0680 - val_accuracy: 0.9926\n",
      "Epoch 54/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.0704 - accuracy: 0.9802 - val_loss: 0.0689 - val_accuracy: 0.9901\n",
      "Epoch 55/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.0699 - accuracy: 0.9765 - val_loss: 0.0839 - val_accuracy: 0.9801\n",
      "Epoch 56/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.0540 - accuracy: 0.9789 - val_loss: 0.0843 - val_accuracy: 0.9901\n",
      "Epoch 57/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.0968 - accuracy: 0.9579 - val_loss: 0.1154 - val_accuracy: 0.9777\n",
      "Epoch 58/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.0787 - accuracy: 0.9752 - val_loss: 0.0979 - val_accuracy: 0.9801\n",
      "Epoch 59/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.0952 - accuracy: 0.9678 - val_loss: 0.1069 - val_accuracy: 0.9727\n",
      "Epoch 60/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.1094 - accuracy: 0.9665 - val_loss: 0.1090 - val_accuracy: 0.9801\n",
      "Epoch 61/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1044 - accuracy: 0.9665 - val_loss: 0.0711 - val_accuracy: 0.9851\n",
      "Epoch 62/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.0896 - accuracy: 0.9727 - val_loss: 0.0737 - val_accuracy: 0.9851\n",
      "Epoch 63/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.0701 - accuracy: 0.9802 - val_loss: 0.0678 - val_accuracy: 0.9801\n",
      "Epoch 64/80\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 0.2022 - accuracy: 0.9492 - val_loss: 0.1226 - val_accuracy: 0.9752\n",
      "Epoch 65/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1445 - accuracy: 0.9653 - val_loss: 0.1152 - val_accuracy: 0.9727\n",
      "Epoch 66/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1067 - accuracy: 0.9641 - val_loss: 0.1092 - val_accuracy: 0.9777\n",
      "Epoch 67/80\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 0.1282 - accuracy: 0.9641 - val_loss: 0.1277 - val_accuracy: 0.9727\n",
      "Epoch 68/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1215 - accuracy: 0.9616 - val_loss: 0.0969 - val_accuracy: 0.9851\n",
      "Epoch 69/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1176 - accuracy: 0.9641 - val_loss: 0.1056 - val_accuracy: 0.9826\n",
      "Epoch 70/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.1876 - accuracy: 0.9554 - val_loss: 0.1366 - val_accuracy: 0.9677\n",
      "Epoch 71/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.2197 - accuracy: 0.9430 - val_loss: 0.2033 - val_accuracy: 0.9653\n",
      "Epoch 72/80\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.1304 - accuracy: 0.9554 - val_loss: 0.1112 - val_accuracy: 0.9702\n",
      "Epoch 73/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.1152 - accuracy: 0.9641 - val_loss: 0.1037 - val_accuracy: 0.9677\n",
      "Epoch 74/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.0854 - accuracy: 0.9740 - val_loss: 0.1270 - val_accuracy: 0.9777\n",
      "Epoch 75/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1052 - accuracy: 0.9765 - val_loss: 0.1466 - val_accuracy: 0.9677\n",
      "Epoch 76/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.0655 - accuracy: 0.9715 - val_loss: 0.1435 - val_accuracy: 0.9653\n",
      "Epoch 77/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.0957 - accuracy: 0.9703 - val_loss: 0.1294 - val_accuracy: 0.9752\n",
      "Epoch 78/80\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 0.1460 - accuracy: 0.9665 - val_loss: 0.1202 - val_accuracy: 0.9826\n",
      "Epoch 79/80\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 0.1719 - accuracy: 0.9529 - val_loss: 0.1272 - val_accuracy: 0.9727\n",
      "Epoch 80/80\n",
      "26/26 [==============================] - 1s 50ms/step - loss: 0.1850 - accuracy: 0.9455 - val_loss: 0.1891 - val_accuracy: 0.9479\n",
      "CNN1D Model Saved\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "skf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "t0 = time.time()\n",
    "# Train and evaluate your model using k-fold cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X_pre, y_enc)):\n",
    "    print(f'Fold {fold+1}')\n",
    "    X_train, y_train = X_pre[train_index], y_enc[train_index]\n",
    "    X_val, y_val = X_pre[val_index], y_enc[val_index]\n",
    "\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'logs/fold_{fold+1}', histogram_freq=1)\n",
    "\n",
    "    CNN1D_Results = CNN1D_Model.fit(X_train, y_train, epochs=80, batch_size=32, validation_data=(X_val, y_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "# # Compute the average validation performance across all k folds\n",
    "# mean_val_score = np.mean(val_scores)\n",
    "# print(f'Average validation accuracy: {mean_val_score}')\n",
    "\n",
    "CNN1D_Model.save(\"Model2_kfold.h5\")\n",
    "print(\"CNN1D Model Saved\")\n",
    "train_hist_m2 = pd.DataFrame(CNN1D_Results.history)\n",
    "train_m2 = round(time.time() - t0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e3491eb595aaf92f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e3491eb595aaf92f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "\n",
    "# CNN1D_Results = CNN1D_Model.fit(\n",
    "#     X_train, y_train, batch_size=32, epochs=120, validation_data=(X_test, y_test)\n",
    "# )\n",
    "\n",
    "# CNN1D_Model.save(\"Model2.h5\")\n",
    "# print(\"CNN1D Model Saved\")\n",
    "# train_hist_m2 = pd.DataFrame(CNN1D_Results.history)\n",
    "# train_m2 = round(time.time() - t0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_hist_m2[[\"loss\", \"val_loss\"]])\n",
    "# plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "# plt.title(\"Loss Per Epochs\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_hist_m2[[\"accuracy\", \"val_accuracy\"]])\n",
    "# plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
    "# plt.title(\"Accuracy Per Epochs\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Finished recording.\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "# Set parameters for audio recording\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "# Create PyAudio object\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open audio stream from microphone\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE,\n",
    "                    input=True, frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"Recording...\")\n",
    "\n",
    "# Create buffer to store audio data\n",
    "frames = []\n",
    "\n",
    "# Record audio for specified number of seconds\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"Finished recording.\")\n",
    "\n",
    "# Stop and close audio stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Write audio data to WAV file\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000022F76F6BC80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Predicted class: 7\n",
      "An-Nasr\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "\n",
    "labels = ['An-Nas', 'Al-Falaq','Al-Fil','Quraish','Al-Maun','Al-Kauthar','Al-Kafirun','An-Nasr','Al-Masad','Al-Ikhlas']\n",
    "\n",
    "# Load audio file and compute MFCC features\n",
    "def compute_mfcc(audio_file):\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    mfcc = np.mean(mfcc.T,axis=0)\n",
    "    return mfcc\n",
    "\n",
    "# Load h5 model and make prediction\n",
    "def predict(audio_file, model_file):\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(model_file)\n",
    "\n",
    "    # Compute MFCC features\n",
    "    mfcc = compute_mfcc(audio_file)\n",
    "\n",
    "    # Reshape MFCC features to match expected input shape of model\n",
    "    mfcc = np.expand_dims(mfcc, axis=0)\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(mfcc)\n",
    "\n",
    "    # Print predicted class\n",
    "    print('Predicted class:', np.argmax(predictions[0]) )\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        if i ==  np.argmax(predictions[0]):\n",
    "            print(labels[i])\n",
    "        \n",
    "        \n",
    "        \n",
    "# Example usage\n",
    "predict('output.wav', 'Model2.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - KWT Transformers Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pathlib\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import tensorflow as tf\n",
    "\n",
    "# from tensorflow.keras.layers.experimental import preprocessing\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras import models\n",
    "# from IPython import display\n",
    "\n",
    "# from utils import mel_features\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# # Set seed for experiment reproducibility\n",
    "# seed = 42\n",
    "# tf.random.set_seed(seed)\n",
    "# np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.transformer import TransformerEncoder, PatchClassEmbedding\n",
    "# from utils.tools import CustomSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model configurations\n",
    "# n_heads = 3 # select kwt 1 2 or 3\n",
    "# d_model = 64 * n_heads\n",
    "# d_ff = d_model * 4\n",
    "# mlp_head_size = 768\n",
    "# dropout = 0.1\n",
    "# activation = tf.nn.gelu\n",
    "# n_layers = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_kwt(transformer, input_size):\n",
    "#     # Input\n",
    "#     inputs = tf.keras.layers.Input(shape=input_size)\n",
    "    \n",
    "#     # Linear Projection of Flattened Patches\n",
    "#     x = tf.keras.layers.Dense(d_model)(inputs)\n",
    "    \n",
    "#     # Position Embedding + Extra learnable class embedding\n",
    "#     x = PatchClassEmbedding(d_model, input_size[0])(x)\n",
    "    \n",
    "#     # Transformer Model\n",
    "#     x = transformer(x)\n",
    "    \n",
    "#     # Take only the Extra Learnable Class\n",
    "#     x = tf.keras.layers.Lambda(lambda x: x[:,0,:])(x)\n",
    "    \n",
    "#     # MLP Head\n",
    "#     x = tf.keras.layers.Dense(mlp_head_size)(x)\n",
    "#     outputs = tf.keras.layers.Dense(len(labels), activation='softmax')(x)\n",
    "    \n",
    "#     return tf.keras.models.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = TransformerEncoder(d_model, n_heads, d_ff, dropout, activation, n_layers)\n",
    "# model = build_kwt(transformer, input_size=(X_train_reshaped.shape[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set some variables\n",
    "# batch_size = 64\n",
    "# n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = CustomSchedule(d_model, warmup_steps=20000.0)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# model.compile(\n",
    "#     optimizer=optimizer,\n",
    "#     loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0.1),\n",
    "#     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     X_train_reshaped, y_train, \n",
    "#     validation_data=(X_test_reshaped, y_test),  \n",
    "#     epochs=n_epochs, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = history.history\n",
    "# plt.plot(history.epoch, metrics['accuracy'], metrics['val_accuracy'])\n",
    "# plt.legend(['accuracy', 'val_accuracy'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = history.history\n",
    "# plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "# plt.legend(['loss', 'val_loss'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(X_test_reshaped, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - CNN-2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN2D_Model = Sequential()\n",
    "# CNN2D_Model.add(\n",
    "#     Conv2D(64, (3, 3), padding=\"same\", activation=\"tanh\", input_shape=(10, 4, 1))\n",
    "# )\n",
    "# CNN2D_Model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# CNN2D_Model.add(Conv2D(128, (3, 3), padding=\"same\", activation=\"tanh\"))\n",
    "# CNN2D_Model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# CNN2D_Model.add(Dropout(0.1))\n",
    "# CNN2D_Model.add(Flatten())\n",
    "# CNN2D_Model.add(Dense(1024, activation=\"tanh\",  kernel_regularizer=regularizers.l1(l=0.01)))\n",
    "# CNN2D_Model.add(Dense(2, activation=\"softmax\"))\n",
    "# CNN2D_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN2D_Model.compile(\n",
    "#     optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "\n",
    "# CNN2D_Results = CNN2D_Model.fit(\n",
    "#     X_train_reshaped, y_train, epochs=100, batch_size=16, validation_data=(X_test_reshaped, y_test)\n",
    "# )\n",
    "\n",
    "# CNN2D_Model.save(\"Model3.h5\")\n",
    "# print(\"CNN2D Model Saved\")\n",
    "# train_hist_m3 = pd.DataFrame(CNN2D_Results.history)\n",
    "# train_m3 = round(time.time() - t0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_hist_m2[[\"loss\", \"val_loss\"]])\n",
    "# plt.legend([\"Loss\", \"Validation Loss\"])\n",
    "# plt.title(\"Loss Per Epochs\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_hist_m2[[\"accuracy\", \"val_accuracy\"]])\n",
    "# plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
    "# plt.title(\"Accuracy Per Epochs\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
